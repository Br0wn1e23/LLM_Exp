{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LaptopBraun\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LaptopBraun\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LaptopBraun\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\fpdf\\__init__.py:39: UserWarning: You have both PyFPDF & fpdf2 installed. Both packages cannot be installed at the same time as they share the same module namespace. To only keep fpdf2, run: pip uninstall --yes pypdf && pip install --upgrade fpdf2\n",
      "  warnings.warn(\n",
      "C:\\Users\\LaptopBraun\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "##Functions and libraries\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from fpdf import FPDF\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "##Point to the local server and the model to be used and set up the stable prompt contents\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:1234/v1\", api_key=\"lm-studio\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "system_prompt =  \"Decide whether the entire prompt you receive is either a positive or a negative movie review. Only answer with a single digit, use '1' for positive or with '0' for negativ. Do not answer with anything else than '1' or '0'.\"\n",
    "system_prompt_RAG = \"Decide whether the entire prompt you receive is either a positive or a negative movie review. Only answer with a single digit, use '1' for positive or with '0' for negativ. Do not answer with anything else than '1' or '0'. Use the ratings of the following similar reviews to make your decision. \"\n",
    "user_prompt_stable =  \"Decide whether the entire prompt you receive is either a positive or a negative movie review. Only answer with a single digit, use '1' for positive or with '0' for negativ. Do not answer with anything else than '1' or '0'. The review goes as follows: \"\n",
    "\n",
    "##Dataset\n",
    "# Decode the review froms the dataset\n",
    "def decode_reviews(sequences):\n",
    "    \n",
    "    return [\n",
    "        \" \".join([reverse_word_index.get(i - 3, \"?\") for i in sequence[1:]])\n",
    "        for sequence in sequences\n",
    "    ]\n",
    "\n",
    "\n",
    "##Normal Prompting procedure\n",
    "# API access to the model\n",
    "def prompt_model(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"llama-3-8b-lexi-uncensored\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": user_prompt_stable + prompt},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "##RAG Magic\n",
    "# Create vector embeddings for the reviews\n",
    "def encode_text_list(text_list):\n",
    "    embeddings = [model.encode(text) for text in text_list]\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# return the indices of the top n relevant reviews for retrieval\n",
    "def calculate_top_n_similarities(prompt, stored_embeddings, top_n=3):\n",
    "    # Encode the new prompt\n",
    "    prompt_embedding = model.encode(prompt)\n",
    "\n",
    "    # Calculate cosine similarity with each stored embedding\n",
    "    similarities = [\n",
    "        1 - cosine(prompt_embedding, emb) for emb in stored_embeddings\n",
    "    ]  # 1 - cosine_distance = cosine_similarity. The higher the value, the more similar the vectors are\n",
    "\n",
    "    # Get the indices of the top n similarities\n",
    "    top_n_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "\n",
    "    return top_n_indices\n",
    "\n",
    "\n",
    "# construct context query for the model\n",
    "def integrate_knowledge(stored_reviews, top_n_indices):\n",
    "    # Define ordinal words for readability\n",
    "    ordinals = [\n",
    "        \"The most similar\",\n",
    "        \"The second most similar\",\n",
    "        \"The third most similar\",\n",
    "        \"The fourth most similar\",\n",
    "        \"The fifth most similar\",\n",
    "        \"The sixth most similar\",\n",
    "        \"The seventh most similar\",\n",
    "        \"The eighth most similar\",\n",
    "        \"The ninth most similar\",\n",
    "        \"The tenth most similar\",\n",
    "    ]\n",
    "    # Construct the formatted string by integrating the reviews in the correct order\n",
    "    integrated_string = system_prompt_RAG\n",
    "    for i, idx in enumerate(top_n_indices):\n",
    "        # Get the ordinal word based on index position, or use \"next\" if ordinals are exceeded\n",
    "        ordinal_word = ordinals[i] if i < len(ordinals) else \"next\"\n",
    "        # Append the formatted review to the integrated string\n",
    "        integrated_string += f\"{ordinal_word} review, {stored_reviews[idx]}\"\n",
    "\n",
    "    return integrated_string.strip()\n",
    "\n",
    "\n",
    "# RAG-prompts for the model\n",
    "def RAG_prompt_model(prompt):\n",
    "    top_n_indices = calculate_top_n_similarities(prompt, stored_embeddings, top_n=3)\n",
    "    integragted_prompt = integrate_knowledge(review_list, top_n_indices)\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"llama-3-8b-lexi-uncensored\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": integragted_prompt,\n",
    "            },\n",
    "            {\"role\": \"user\", \n",
    "             \"content\": user_prompt_stable+ prompt},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "# Generate RAG document for AnythingLLM\n",
    "def write_reviews_to_txt(reviews, ratings, output_filename=\"movie_reviews.txt\"):\n",
    "    # Check that both lists have the same length\n",
    "    if len(reviews) != len(ratings):\n",
    "        raise ValueError(\"The number of reviews and ratings must be the same.\")\n",
    "\n",
    "    try:\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            # Iterate through reviews and ratings\n",
    "            for review, rating in zip(reviews, ratings):\n",
    "                # Format the review text\n",
    "                formatted_review = f'The review: \"{review}\" was rated \"{rating}\".\\n'\n",
    "\n",
    "                # Write the formatted review to the file\n",
    "                file.write(formatted_review)\n",
    "                file.write(\"\\n\")  # Add a blank line between reviews\n",
    "\n",
    "        print(f\"Text file '{output_filename}' has been created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving text file: {e}\")\n",
    "\n",
    "\n",
    "# generate list of reviews and ratings\n",
    "def store_reviews_with_ratings(reviews, ratings):\n",
    "    # Check that both lists have the same length\n",
    "    if len(reviews) != len(ratings):\n",
    "        raise ValueError(\"The number of text inputs and ratings must be the same.\")\n",
    "\n",
    "    # List to store the formatted reviews with ratings\n",
    "    formatted_reviews = []\n",
    "\n",
    "    # Iterate through reviews and ratings\n",
    "    for review, rating in zip(reviews, ratings):\n",
    "        # Format the review text\n",
    "        formatted_review = f'the review: \"{review}\" was rated \"{rating}\".'\n",
    "\n",
    "        # Append the formatted review to the list\n",
    "        formatted_reviews.append(formatted_review)\n",
    "\n",
    "    return formatted_reviews\n",
    "\n",
    "\n",
    "# Working in bigger batches with progress feedback\n",
    "def batch_prompt_model(prompts, mode=\"normal\"):\n",
    "    results = []\n",
    "    for p in prompts:\n",
    "        if mode == \"normal\":\n",
    "            results.append(prompt_model(p))\n",
    "        elif mode == \"RAG\":\n",
    "            results.append(RAG_prompt_model(p))\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid mode '{mode}'.\")\n",
    "        if len(results) % 10 == 0:\n",
    "            print(f\"Completed {len(results)} prompts.\")\n",
    "    return results\n",
    "\n",
    "\n",
    "##Handling unusual outputs\n",
    "# Helper funtion to handle weird outputs\n",
    "def convert_outputs(strings):\n",
    "    result = []\n",
    "    weird = 0\n",
    "    skipped = 0\n",
    "    skipped_indices = []\n",
    "\n",
    "    for i, s in enumerate(strings):\n",
    "        if s != \"1\" and s != \"0\":\n",
    "            weird += 1\n",
    "        # Filter out any characters that are not '1' or '0'\n",
    "        cleaned = \"\".join([char for char in s if char in \"10\"])\n",
    "\n",
    "        # Convert to integer if the cleaned string is exactly \"1\" or \"0\"\n",
    "        if cleaned == \"1\":\n",
    "            result.append(1)\n",
    "        elif cleaned == \"0\":\n",
    "            result.append(0)\n",
    "        else:\n",
    "            # Handle unexpected cases if needed; here we skip them\n",
    "            print(f\"Warning: Unrecognized format '{s}', skipping.\")\n",
    "            print(\"\")\n",
    "            skipped += 1\n",
    "            skipped_indices.append(i)\n",
    "    if weird > 0:\n",
    "        print(f\"This batch query produced {weird} weird outputs.\")\n",
    "    if skipped > 0:\n",
    "        print(\n",
    "            f\"Additionally, it skipped {skipped} outputs that did not contain 1 or 0 at all.\"\n",
    "        )\n",
    "    return result, skipped_indices\n",
    "\n",
    "\n",
    "# delete the skipped indices from the list if needed\n",
    "def clean_y(y, skipped_indices):\n",
    "    if len(skipped_indices) > 0:\n",
    "        print(f\"Warning: removing {len(skipped_indices)} skipped outputs from y.\")\n",
    "        return [y[i] for i in range(len(y)) if i not in skipped_indices]\n",
    "    else:\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test normal, non-augemented prompting first.\n",
    "The dataset contains movie reviews either rated positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Dataset\n",
    "# IMDB dataset preparation\n",
    "# Reverse the word index to create a mapping from integer indices to words\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
    "\n",
    "#Load dataset\n",
    "(train_x, train_y), (test_x, test_y) = imdb.load_data(num_words=100000, seed=None)\n",
    "set_size = 10\n",
    "x = decode_reviews(test_x[:set_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 10 prompts.\n",
      "Accuracy of standard prompting: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Select a subset of the dataset and try the basic model\n",
    "cats, ids = convert_outputs(batch_prompt_model(x))\n",
    "valid_test_y = clean_y(test_y[:set_size], ids)\n",
    "print(f\"Accuracy of standard prompting: {accuracy_score(valid_test_y, cats)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we will test the simple RAG implementation set-up in this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate RAG Knowledge-Base\n",
    "RAG_size = 1000\n",
    "train_y_RAG = [\"positive\" if i == 1 else \"negative\" for i in train_y]\n",
    "review_list = store_reviews_with_ratings(decode_reviews(train_x[:RAG_size]), train_y_RAG[:RAG_size])\n",
    "stored_embeddings = encode_text_list(review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 10 prompts.\n",
      "Accuracy of RAG prompting: 1.0\n"
     ]
    }
   ],
   "source": [
    "cats, ids = convert_outputs(batch_prompt_model(x, mode=\"RAG\"))\n",
    "valid_test_y = clean_y(test_y[:set_size], ids)\n",
    "print(f\"Accuracy of RAG prompting: {accuracy_score(valid_test_y, cats)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
