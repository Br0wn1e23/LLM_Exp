{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LaptopBraun\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "##Functions and libraries\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from fpdf import FPDF\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "##Point to the local server and the model to be used and set up the stable prompt contents\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:1234/v1\", api_key=\"lm-studio\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "system_prompt =  \"Rate how positive or a negative a clothing article review is. Only answer with a single digit from 1 to 5, use '1' for negative, '2' for somewhat negative, '3' for neutral, '4' for somewhat positive and '5' for positive. Do not answer with anything else than '1' or '2' or '3' or '4' or '5'.\"\n",
    "system_prompt_RAG = \"Rate how positive or a negative a clothing article review is. Only answer with a single digit from 1 to 5, use '1' for negative, '2' for somewhat negative, '3' for neutral, '4' for somewhat positive and '5' for positive. Do not answer with anything else than '1' or '2' or '3' or '4' or '5'. Use the ratings of the following similar reviews to make your decision. \"\n",
    "user_prompt_stable =  \"Rate how positive or a negative a clothing article review is. Only answer with a single digit from 1 to 5, use '1' for negative, '2' for somewhat negative, '3' for neutral, '4' for somewhat positive and '5' for positive. Do not answer with anything else than '1' or '2' or '3' or '4' or '5'. The review goes as follows: \"\n",
    "\n",
    "##Dataset\n",
    "# Decode the review froms the dataset, only relevant for IMDB\n",
    "# def decode_reviews(sequences):\n",
    "    \n",
    "#     return [\n",
    "#         \" \".join([reverse_word_index.get(i - 3, \"?\") for i in sequence[1:]])\n",
    "#         for sequence in sequences\n",
    "#     ]\n",
    "\n",
    "\n",
    "##Normal Prompting procedure\n",
    "# API access to the model\n",
    "def prompt_model(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"llama-3-8b-lexi-uncensored\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": user_prompt_stable + str(prompt)},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "##RAG Magic\n",
    "# Create vector embeddings for the reviews\n",
    "def encode_text_list(text_list):\n",
    "    embeddings = [model.encode(text) for text in text_list]\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# return the indices of the top n relevant reviews for retrieval\n",
    "def calculate_top_n_similarities(prompt, stored_embeddings, top_n=3):\n",
    "    # Encode the new prompt\n",
    "    prompt_embedding = model.encode(prompt)\n",
    "\n",
    "    # Calculate cosine similarity with each stored embedding\n",
    "    similarities = [\n",
    "        1 - cosine(prompt_embedding, emb) for emb in stored_embeddings\n",
    "    ]  # 1 - cosine_distance = cosine_similarity. The higher the value, the more similar the vectors are\n",
    "\n",
    "    # Get the indices of the top n similarities\n",
    "    top_n_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "\n",
    "    return top_n_indices\n",
    "\n",
    "\n",
    "# construct context query for the model\n",
    "def integrate_knowledge(stored_reviews, top_n_indices):\n",
    "    # Define ordinal words for readability\n",
    "    ordinals = [\n",
    "        \"The most similar\",\n",
    "        \"The second most similar\",\n",
    "        \"The third most similar\",\n",
    "        \"The fourth most similar\",\n",
    "        \"The fifth most similar\",\n",
    "        \"The sixth most similar\",\n",
    "        \"The seventh most similar\",\n",
    "        \"The eighth most similar\",\n",
    "        \"The ninth most similar\",\n",
    "        \"The tenth most similar\",\n",
    "    ]\n",
    "    # Construct the formatted string by integrating the reviews in the correct order\n",
    "    integrated_string = system_prompt_RAG\n",
    "    for i, idx in enumerate(top_n_indices):\n",
    "        # Get the ordinal word based on index position, or use \"next\" if ordinals are exceeded\n",
    "        ordinal_word = ordinals[i] if i < len(ordinals) else \"next\"\n",
    "        # Append the formatted review to the integrated string\n",
    "        integrated_string += f\"{ordinal_word} review, {stored_reviews[idx]}\"\n",
    "\n",
    "    return integrated_string.strip()\n",
    "\n",
    "\n",
    "# RAG-prompts for the model\n",
    "def RAG_prompt_model(prompt, n_known_reviews=5):\n",
    "    top_n_indices = calculate_top_n_similarities(prompt, stored_embeddings, top_n=n_known_reviews)\n",
    "    integragted_prompt = integrate_knowledge(review_list, top_n_indices)\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"llama-3-8b-lexi-uncensored\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": integragted_prompt,\n",
    "            },\n",
    "            {\"role\": \"user\", \n",
    "             \"content\": user_prompt_stable+ str(prompt)},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "# Generate RAG document for AnythingLLM\n",
    "# def write_reviews_to_txt(reviews, ratings, output_filename=\"movie_reviews.txt\"):\n",
    "#     # Check that both lists have the same length\n",
    "#     if len(reviews) != len(ratings):\n",
    "#         raise ValueError(\"The number of reviews and ratings must be the same.\")\n",
    "\n",
    "#     try:\n",
    "#         with open(output_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "#             # Iterate through reviews and ratings\n",
    "#             for review, rating in zip(reviews, ratings):\n",
    "#                 # Format the review text\n",
    "#                 formatted_review = f'The review: \"{review}\" was rated \"{rating}\".\\n'\n",
    "\n",
    "#                 # Write the formatted review to the file\n",
    "#                 file.write(formatted_review)\n",
    "#                 file.write(\"\\n\")  # Add a blank line between reviews\n",
    "\n",
    "#         print(f\"Text file '{output_filename}' has been created successfully.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error saving text file: {e}\")\n",
    "\n",
    "\n",
    "# generate list of reviews and ratings\n",
    "def store_reviews_with_ratings(reviews, ratings):\n",
    "    # Check that both lists have the same length\n",
    "    if len(reviews) != len(ratings):\n",
    "        raise ValueError(\"The number of text inputs and ratings must be the same.\")\n",
    "\n",
    "    # List to store the formatted reviews with ratings\n",
    "    formatted_reviews = []\n",
    "\n",
    "    # Iterate through reviews and ratings\n",
    "    for review, rating in zip(reviews, ratings):\n",
    "        # Format the review text\n",
    "        formatted_review = f'the review: \"{review}\" was rated \"{rating}\".'\n",
    "\n",
    "        # Append the formatted review to the list\n",
    "        formatted_reviews.append(formatted_review)\n",
    "\n",
    "    return formatted_reviews\n",
    "\n",
    "\n",
    "# Working in bigger batches with progress feedback\n",
    "def batch_prompt_model(prompts, mode=\"normal\"):\n",
    "    results = []\n",
    "    for p in prompts:\n",
    "        if mode == \"normal\":\n",
    "            results.append(prompt_model(p))\n",
    "        elif mode == \"RAG\":\n",
    "            results.append(RAG_prompt_model(p))\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid mode '{mode}'.\")\n",
    "        if len(results) % 10 == 0:\n",
    "            print(f\"Completed {len(results)} prompts.\")\n",
    "    return results\n",
    "\n",
    "\n",
    "##Handling unusual outputs\n",
    "# Helper funtion to handle weird outputs\n",
    "def convert_outputs(strings):\n",
    "    result = []\n",
    "    weird = 0\n",
    "    skipped = 0\n",
    "    skipped_indices = []\n",
    "\n",
    "    for i, s in enumerate(strings):\n",
    "        if s not in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n",
    "            weird += 1\n",
    "        # Filter out any characters that are not '1' or '0'\n",
    "        cleaned = \"\".join([char for char in s if char in \"12345\"])\n",
    "\n",
    "        # Convert to integer if the cleaned string is exactly \"1\" or \"0\"\n",
    "        if cleaned == \"1\":\n",
    "            result.append(1)\n",
    "        elif cleaned == \"2\":\n",
    "            result.append(2)\n",
    "        elif cleaned == \"3\":\n",
    "            result.append(3)\n",
    "        elif cleaned == \"4\":\n",
    "            result.append(4)\n",
    "        elif cleaned == \"5\":\n",
    "            result.append(5)\n",
    "        else:\n",
    "            # Handle unexpected cases if needed; here we skip them\n",
    "            print(f\"Warning: Unrecognized format '{s}', skipping.\")\n",
    "            print(\"\")\n",
    "            skipped += 1\n",
    "            skipped_indices.append(i)\n",
    "    if weird > 0:\n",
    "        print(f\"This batch query produced {weird} weird outputs.\")\n",
    "    if skipped > 0:\n",
    "        print(\n",
    "            f\"Additionally, it skipped {skipped} outputs that did not contain 1 or 0 at all.\"\n",
    "        )\n",
    "    return result, skipped_indices\n",
    "\n",
    "\n",
    "# delete the skipped indices from the list if needed\n",
    "def clean_y(y, skipped_indices):\n",
    "    if len(skipped_indices) > 0:\n",
    "        print(f\"Warning: removing {len(skipped_indices)} skipped outputs from y.\")\n",
    "        return [y[i] for i in range(len(y)) if i not in skipped_indices]\n",
    "    else:\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test normal, non-augemented prompting first.\n",
    "The dataset contains movie reviews either rated positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Dataset\n",
    "x_split = 5000\n",
    "reviews_data = pd.read_csv(\"Clothing Reviews.csv\").dropna(subset=[\"Review Text\",\"Rating\"]).sample(frac=1).reset_index(drop=True) #shuffle the dataset every time\n",
    "train_x, train_y, test_x, test_y = reviews_data[\"Review Text\"][:x_split].values, reviews_data[\"Rating\"][:x_split].values, reviews_data[\"Review Text\"][x_split:].values, reviews_data[\"Rating\"][x_split:].values\n",
    "#set_size = len(reviews_data)-x_split\n",
    "set_size = 500\n",
    "x = test_x[:set_size]\n",
    "y = test_y[:set_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select a subset of the dataset and try the basic model\n",
    "#cats, ids = convert_outputs(batch_prompt_model(x))\n",
    "# valid_test_y = clean_y(y, ids)\n",
    "# print(f\"Weighted F1-Score of standard prompting: {f1_score(valid_test_y, cats, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we will test the simple RAG implementation set-up in this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate RAG Knowledge-Base\n",
    "# RAG_size = x_split #full size\n",
    "# review_list = store_reviews_with_ratings(train_x[:RAG_size], train_y[:RAG_size])\n",
    "# stored_embeddings = encode_text_list(review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cats, ids = convert_outputs(batch_prompt_model(x, mode=\"RAG\"))\n",
    "#valid_test_y = clean_y(test_y[:set_size], ids)\n",
    "#print(f\"Weighted F1-Score of RAG prompting: {f1_score(valid_test_y, cats, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to benchmark this performance against a transfer learning approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LaptopBraun\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen BERT layer: embeddings.word_embeddings.weight\n",
      "Frozen BERT layer: embeddings.position_embeddings.weight\n",
      "Frozen BERT layer: embeddings.LayerNorm.weight\n",
      "Frozen BERT layer: embeddings.LayerNorm.bias\n",
      "Frozen BERT layer: transformer.layer.0.attention.q_lin.weight\n",
      "Frozen BERT layer: transformer.layer.0.attention.q_lin.bias\n",
      "Frozen BERT layer: transformer.layer.0.attention.k_lin.weight\n",
      "Frozen BERT layer: transformer.layer.0.attention.k_lin.bias\n",
      "Frozen BERT layer: transformer.layer.0.attention.v_lin.weight\n",
      "Frozen BERT layer: transformer.layer.0.attention.v_lin.bias\n",
      "Frozen BERT layer: transformer.layer.0.attention.out_lin.weight\n",
      "Frozen BERT layer: transformer.layer.0.attention.out_lin.bias\n",
      "Frozen BERT layer: transformer.layer.0.sa_layer_norm.weight\n",
      "Frozen BERT layer: transformer.layer.0.sa_layer_norm.bias\n",
      "Frozen BERT layer: transformer.layer.0.ffn.lin1.weight\n",
      "Frozen BERT layer: transformer.layer.0.ffn.lin1.bias\n",
      "Frozen BERT layer: transformer.layer.0.ffn.lin2.weight\n",
      "Frozen BERT layer: transformer.layer.0.ffn.lin2.bias\n",
      "Frozen BERT layer: transformer.layer.0.output_layer_norm.weight\n",
      "Frozen BERT layer: transformer.layer.0.output_layer_norm.bias\n",
      "Frozen BERT layer: transformer.layer.1.attention.q_lin.weight\n",
      "Frozen BERT layer: transformer.layer.1.attention.q_lin.bias\n",
      "Frozen BERT layer: transformer.layer.1.attention.k_lin.weight\n",
      "Frozen BERT layer: transformer.layer.1.attention.k_lin.bias\n",
      "Frozen BERT layer: transformer.layer.1.attention.v_lin.weight\n",
      "Frozen BERT layer: transformer.layer.1.attention.v_lin.bias\n",
      "Frozen BERT layer: transformer.layer.1.attention.out_lin.weight\n",
      "Frozen BERT layer: transformer.layer.1.attention.out_lin.bias\n",
      "Frozen BERT layer: transformer.layer.1.sa_layer_norm.weight\n",
      "Frozen BERT layer: transformer.layer.1.sa_layer_norm.bias\n",
      "Frozen BERT layer: transformer.layer.1.ffn.lin1.weight\n",
      "Frozen BERT layer: transformer.layer.1.ffn.lin1.bias\n",
      "Frozen BERT layer: transformer.layer.1.ffn.lin2.weight\n",
      "Frozen BERT layer: transformer.layer.1.ffn.lin2.bias\n",
      "Frozen BERT layer: transformer.layer.1.output_layer_norm.weight\n",
      "Frozen BERT layer: transformer.layer.1.output_layer_norm.bias\n",
      "Frozen BERT layer: transformer.layer.2.attention.q_lin.weight\n",
      "Frozen BERT layer: transformer.layer.2.attention.q_lin.bias\n",
      "Frozen BERT layer: transformer.layer.2.attention.k_lin.weight\n",
      "Frozen BERT layer: transformer.layer.2.attention.k_lin.bias\n",
      "Frozen BERT layer: transformer.layer.2.attention.v_lin.weight\n",
      "Frozen BERT layer: transformer.layer.2.attention.v_lin.bias\n",
      "Frozen BERT layer: transformer.layer.2.attention.out_lin.weight\n",
      "Frozen BERT layer: transformer.layer.2.attention.out_lin.bias\n",
      "Frozen BERT layer: transformer.layer.2.sa_layer_norm.weight\n",
      "Frozen BERT layer: transformer.layer.2.sa_layer_norm.bias\n",
      "Frozen BERT layer: transformer.layer.2.ffn.lin1.weight\n",
      "Frozen BERT layer: transformer.layer.2.ffn.lin1.bias\n",
      "Frozen BERT layer: transformer.layer.2.ffn.lin2.weight\n",
      "Frozen BERT layer: transformer.layer.2.ffn.lin2.bias\n",
      "Frozen BERT layer: transformer.layer.2.output_layer_norm.weight\n",
      "Frozen BERT layer: transformer.layer.2.output_layer_norm.bias\n",
      "Frozen BERT layer: transformer.layer.3.attention.q_lin.weight\n",
      "Frozen BERT layer: transformer.layer.3.attention.q_lin.bias\n",
      "Frozen BERT layer: transformer.layer.3.attention.k_lin.weight\n",
      "Frozen BERT layer: transformer.layer.3.attention.k_lin.bias\n",
      "Frozen BERT layer: transformer.layer.3.attention.v_lin.weight\n",
      "Frozen BERT layer: transformer.layer.3.attention.v_lin.bias\n",
      "Frozen BERT layer: transformer.layer.3.attention.out_lin.weight\n",
      "Frozen BERT layer: transformer.layer.3.attention.out_lin.bias\n",
      "Frozen BERT layer: transformer.layer.3.sa_layer_norm.weight\n",
      "Frozen BERT layer: transformer.layer.3.sa_layer_norm.bias\n",
      "Frozen BERT layer: transformer.layer.3.ffn.lin1.weight\n",
      "Frozen BERT layer: transformer.layer.3.ffn.lin1.bias\n",
      "Frozen BERT layer: transformer.layer.3.ffn.lin2.weight\n",
      "Frozen BERT layer: transformer.layer.3.ffn.lin2.bias\n",
      "Frozen BERT layer: transformer.layer.3.output_layer_norm.weight\n",
      "Frozen BERT layer: transformer.layer.3.output_layer_norm.bias\n",
      "Frozen BERT layer: transformer.layer.4.attention.q_lin.weight\n",
      "Frozen BERT layer: transformer.layer.4.attention.q_lin.bias\n",
      "Frozen BERT layer: transformer.layer.4.attention.k_lin.weight\n",
      "Frozen BERT layer: transformer.layer.4.attention.k_lin.bias\n",
      "Frozen BERT layer: transformer.layer.4.attention.v_lin.weight\n",
      "Frozen BERT layer: transformer.layer.4.attention.v_lin.bias\n",
      "Frozen BERT layer: transformer.layer.4.attention.out_lin.weight\n",
      "Frozen BERT layer: transformer.layer.4.attention.out_lin.bias\n",
      "Frozen BERT layer: transformer.layer.4.sa_layer_norm.weight\n",
      "Frozen BERT layer: transformer.layer.4.sa_layer_norm.bias\n",
      "Frozen BERT layer: transformer.layer.4.ffn.lin1.weight\n",
      "Frozen BERT layer: transformer.layer.4.ffn.lin1.bias\n",
      "Frozen BERT layer: transformer.layer.4.ffn.lin2.weight\n",
      "Frozen BERT layer: transformer.layer.4.ffn.lin2.bias\n",
      "Frozen BERT layer: transformer.layer.4.output_layer_norm.weight\n",
      "Frozen BERT layer: transformer.layer.4.output_layer_norm.bias\n",
      "Frozen BERT layer: transformer.layer.5.attention.q_lin.weight\n",
      "Frozen BERT layer: transformer.layer.5.attention.q_lin.bias\n",
      "Frozen BERT layer: transformer.layer.5.attention.k_lin.weight\n",
      "Frozen BERT layer: transformer.layer.5.attention.k_lin.bias\n",
      "Frozen BERT layer: transformer.layer.5.attention.v_lin.weight\n",
      "Frozen BERT layer: transformer.layer.5.attention.v_lin.bias\n",
      "Frozen BERT layer: transformer.layer.5.attention.out_lin.weight\n",
      "Frozen BERT layer: transformer.layer.5.attention.out_lin.bias\n",
      "Frozen BERT layer: transformer.layer.5.sa_layer_norm.weight\n",
      "Frozen BERT layer: transformer.layer.5.sa_layer_norm.bias\n",
      "Frozen BERT layer: transformer.layer.5.ffn.lin1.weight\n",
      "Frozen BERT layer: transformer.layer.5.ffn.lin1.bias\n",
      "Frozen BERT layer: transformer.layer.5.ffn.lin2.weight\n",
      "Frozen BERT layer: transformer.layer.5.ffn.lin2.bias\n",
      "Frozen BERT layer: transformer.layer.5.output_layer_norm.weight\n",
      "Frozen BERT layer: transformer.layer.5.output_layer_norm.bias\n",
      "Frozen layer: model.embeddings.word_embeddings.weight with 23440896 parameters\n",
      "Frozen layer: model.embeddings.position_embeddings.weight with 393216 parameters\n",
      "Frozen layer: model.embeddings.LayerNorm.weight with 768 parameters\n",
      "Frozen layer: model.embeddings.LayerNorm.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.0.attention.q_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.0.attention.q_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.0.attention.k_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.0.attention.k_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.0.attention.v_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.0.attention.v_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.0.attention.out_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.0.attention.out_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.0.sa_layer_norm.weight with 768 parameters\n",
      "Frozen layer: model.transformer.layer.0.sa_layer_norm.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.0.ffn.lin1.weight with 2359296 parameters\n",
      "Frozen layer: model.transformer.layer.0.ffn.lin1.bias with 3072 parameters\n",
      "Frozen layer: model.transformer.layer.0.ffn.lin2.weight with 2359296 parameters\n",
      "Frozen layer: model.transformer.layer.0.ffn.lin2.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.0.output_layer_norm.weight with 768 parameters\n",
      "Frozen layer: model.transformer.layer.0.output_layer_norm.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.1.attention.q_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.1.attention.q_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.1.attention.k_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.1.attention.k_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.1.attention.v_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.1.attention.v_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.1.attention.out_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.1.attention.out_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.1.sa_layer_norm.weight with 768 parameters\n",
      "Frozen layer: model.transformer.layer.1.sa_layer_norm.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.1.ffn.lin1.weight with 2359296 parameters\n",
      "Frozen layer: model.transformer.layer.1.ffn.lin1.bias with 3072 parameters\n",
      "Frozen layer: model.transformer.layer.1.ffn.lin2.weight with 2359296 parameters\n",
      "Frozen layer: model.transformer.layer.1.ffn.lin2.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.1.output_layer_norm.weight with 768 parameters\n",
      "Frozen layer: model.transformer.layer.1.output_layer_norm.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.2.attention.q_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.2.attention.q_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.2.attention.k_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.2.attention.k_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.2.attention.v_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.2.attention.v_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.2.attention.out_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.2.attention.out_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.2.sa_layer_norm.weight with 768 parameters\n",
      "Frozen layer: model.transformer.layer.2.sa_layer_norm.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.2.ffn.lin1.weight with 2359296 parameters\n",
      "Frozen layer: model.transformer.layer.2.ffn.lin1.bias with 3072 parameters\n",
      "Frozen layer: model.transformer.layer.2.ffn.lin2.weight with 2359296 parameters\n",
      "Frozen layer: model.transformer.layer.2.ffn.lin2.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.2.output_layer_norm.weight with 768 parameters\n",
      "Frozen layer: model.transformer.layer.2.output_layer_norm.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.3.attention.q_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.3.attention.q_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.3.attention.k_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.3.attention.k_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.3.attention.v_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.3.attention.v_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.3.attention.out_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.3.attention.out_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.3.sa_layer_norm.weight with 768 parameters\n",
      "Frozen layer: model.transformer.layer.3.sa_layer_norm.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.3.ffn.lin1.weight with 2359296 parameters\n",
      "Frozen layer: model.transformer.layer.3.ffn.lin1.bias with 3072 parameters\n",
      "Frozen layer: model.transformer.layer.3.ffn.lin2.weight with 2359296 parameters\n",
      "Frozen layer: model.transformer.layer.3.ffn.lin2.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.3.output_layer_norm.weight with 768 parameters\n",
      "Frozen layer: model.transformer.layer.3.output_layer_norm.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.4.attention.q_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.4.attention.q_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.4.attention.k_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.4.attention.k_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.4.attention.v_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.4.attention.v_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.4.attention.out_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.4.attention.out_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.4.sa_layer_norm.weight with 768 parameters\n",
      "Frozen layer: model.transformer.layer.4.sa_layer_norm.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.4.ffn.lin1.weight with 2359296 parameters\n",
      "Frozen layer: model.transformer.layer.4.ffn.lin1.bias with 3072 parameters\n",
      "Frozen layer: model.transformer.layer.4.ffn.lin2.weight with 2359296 parameters\n",
      "Frozen layer: model.transformer.layer.4.ffn.lin2.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.4.output_layer_norm.weight with 768 parameters\n",
      "Frozen layer: model.transformer.layer.4.output_layer_norm.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.5.attention.q_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.5.attention.q_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.5.attention.k_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.5.attention.k_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.5.attention.v_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.5.attention.v_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.5.attention.out_lin.weight with 589824 parameters\n",
      "Frozen layer: model.transformer.layer.5.attention.out_lin.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.5.sa_layer_norm.weight with 768 parameters\n",
      "Frozen layer: model.transformer.layer.5.sa_layer_norm.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.5.ffn.lin1.weight with 2359296 parameters\n",
      "Frozen layer: model.transformer.layer.5.ffn.lin1.bias with 3072 parameters\n",
      "Frozen layer: model.transformer.layer.5.ffn.lin2.weight with 2359296 parameters\n",
      "Frozen layer: model.transformer.layer.5.ffn.lin2.bias with 768 parameters\n",
      "Frozen layer: model.transformer.layer.5.output_layer_norm.weight with 768 parameters\n",
      "Frozen layer: model.transformer.layer.5.output_layer_norm.bias with 768 parameters\n",
      "Trainable layer: dense_layer.weight with 24576 parameters\n",
      "Trainable layer: dense_layer.bias with 32 parameters\n",
      "Trainable layer: classifier.weight with 160 parameters\n",
      "Trainable layer: classifier.bias with 5 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LaptopBraun\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Define the model architecture class\n",
    "class MulticlassTextClassificationModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(MulticlassTextClassificationModel, self).__init__()\n",
    "        # Load the pre-trained model (e.g., BERT, RoBERTa, etc.)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Add a larger dense layer (5000 neurons)\n",
    "        self.dense_layer = nn.Linear(self.model.config.hidden_size, 32)\n",
    "        self.relu = nn.ReLU()  # Activation function for the dense layer\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Linear(32, num_labels)  # This layer now takes 5000 neurons as input\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # Get model outputs\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use [CLS] token output for classification (outputs.last_hidden_state[:, 0, :])\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Pass the CLS token output through the dense layer\n",
    "        dense_output = self.dense_layer(cls_output)\n",
    "        dense_output = self.relu(dense_output)  # Apply ReLU activation\n",
    "        \n",
    "        # Classifier layer to get the final logits\n",
    "        logits = self.classifier(dense_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Compute loss using CrossEntropyLoss\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "#Definfe the dataset structure class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_data, labels):\n",
    "        self.input_ids = tokenized_data[\"input_ids\"]\n",
    "        self.attention_mask = tokenized_data[\"attention_mask\"]\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            # Return a new CustomDataset instance for a slice\n",
    "            return CustomDataset(\n",
    "                tokenized_data={\n",
    "                    \"input_ids\": self.input_ids[idx],\n",
    "                    \"attention_mask\": self.attention_mask[idx]\n",
    "                },\n",
    "                labels=self.labels[idx] -1 # Adjust labels to start from 0\n",
    "            )\n",
    "        else:\n",
    "            # Return a single sample as usual\n",
    "            return {\n",
    "                \"input_ids\": self.input_ids[idx],\n",
    "                \"attention_mask\": self.attention_mask[idx],\n",
    "                \"labels\": self.labels[idx] -1 # Adjust labels to start from 0\n",
    "            }\n",
    "    def __str__(self):\n",
    "        # Display a summary of the dataset\n",
    "        return (\n",
    "            f\"CustomDataset with {len(self)} samples\\n\"\n",
    "            f\"Input IDs: {self.input_ids.shape}\\n\"\n",
    "            f\"Attention Masks: {self.attention_mask.shape}\\n\"\n",
    "            f\"Labels: {self.labels.shape}\"\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        # Display more details about the dataset for debugging\n",
    "        content_preview = {\n",
    "            \"input_ids\": self.input_ids[:3].tolist(),\n",
    "            \"attention_mask\": self.attention_mask[:3].tolist(),\n",
    "            \"labels\": self.labels[:3].tolist(),\n",
    "        }\n",
    "        return (\n",
    "            f\"CustomDataset(length={len(self)})\\n\"\n",
    "            f\"Preview of data:\\n{content_preview}\"\n",
    "        )\n",
    "        \n",
    "# Load the F1 metric from the evaluate library\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, label_ids = eval_pred  # Unpack EvalPrediction object\n",
    "    \n",
    "    # Convert logits to predicted class labels\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Compute weighted F1 score\n",
    "    results = f1_metric.compute(predictions=preds, references=label_ids, average=\"weighted\")\n",
    "    \n",
    "    return results\n",
    "#Dataset prepraration\n",
    "eval_set_size = len(train_x) // 20  # 20% of the training set for evaluation\n",
    "max_length = 200 #max text token length\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the text column for the train set\n",
    "tokenized_train = tokenizer(\n",
    "    train_x[eval_set_size:].tolist(),\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_length,\n",
    "    return_tensors=\"pt\"  # Directly return PyTorch tensors\n",
    ")\n",
    "# Add the labels as tensors\n",
    "train_labels = torch.tensor(train_y[eval_set_size:])\n",
    "train_dataset = CustomDataset(tokenized_train, train_labels)\n",
    "\n",
    "\n",
    "# Tokenize the text column for the eval set\n",
    "tokenized_eval = tokenizer(\n",
    "    test_x[:eval_set_size].tolist(),\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    max_length=max_length,\n",
    "    return_tensors=\"pt\"  # Directly return PyTorch tensors\n",
    ")\n",
    "# Add the labels as tensors\n",
    "eval_labels = torch.tensor(train_y[:eval_set_size])\n",
    "eval_dataset = CustomDataset(tokenized_eval, eval_labels)\n",
    "#Define the model and training arguments\n",
    "model = MulticlassTextClassificationModel(model_name, num_labels=5)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", num_train_epochs=3, per_device_train_batch_size=380, evaluation_strategy=\"epoch\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "# Freeze BERT layers (set requires_grad=False for all parameters in the BERT model)\n",
    "for param in model.model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the final custom layers (dense layer and classifier)\n",
    "for param in model.dense_layer.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Check if BERT layers are frozen\n",
    "for name, param in model.model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable BERT layer: {name}\")\n",
    "    else:\n",
    "        print(f\"Frozen BERT layer: {name}\")\n",
    "\n",
    "# Check if custom layers are trainable\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable layer: {name} with {param.numel()} parameters\")\n",
    "    else:\n",
    "        print(f\"Frozen layer: {name} with {param.numel()} parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3262431dec5462ba2673a4d97943be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
